# -*- coding: utf-8 -*-
"""Lstm3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Zgd7j37BPvliQIF6xT98oUJmsj3RBSk4
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# =========================
# Reproducibility
# =========================
tf.random.set_seed(42)
np.random.seed(42)

# =========================
# Sequence creator (EXOGENOUS)
# =========================
def create_exogenous_sequences(X, y, look_back=24):
    Xs, Ys = [], []
    for i in range(len(X) - look_back):
        Xs.append(X[i:i + look_back])
        Ys.append(y[i + look_back])
    return np.array(Xs), np.array(Ys)

# =========================
# Load training data
# =========================
df = pd.read_csv("Clean_file.csv")
df["time"] = pd.to_datetime(df["time"])

# =========================
# Time features
# =========================
df["hour"] = df["time"].dt.hour
df["doy"]  = df["time"].dt.dayofyear

df["hour_sin"] = np.sin(2*np.pi*df["hour"]/24)
df["hour_cos"] = np.cos(2*np.pi*df["hour"]/24)
df["doy_sin"]  = np.sin(2*np.pi*df["doy"]/365)
df["doy_cos"]  = np.cos(2*np.pi*df["doy"]/365)

df = df.drop(columns=["time", "hour", "doy"])

# =========================
# GTI preprocessing (physics-aware)
# =========================
df["gti"] = df["gti"].clip(lower=0)
df["gti_log"] = np.log1p(df["gti"])
df = df.drop(columns=["gti"])

df = df.replace([np.inf, -np.inf], np.nan).ffill().bfill()

# =========================
# Target scaling
# =========================
target_col = "P_norm"
scaler_y = MinMaxScaler()
y_scaled = scaler_y.fit_transform(df[[target_col]]).flatten()

# =========================
# Feature matrix
# =========================
feature_cols = [
    "temp_F",
    "gti_log",
    "hour_sin",
    "hour_cos",
    "doy_sin",
    "doy_cos"
]

scaler_X = MinMaxScaler()
X_scaled = scaler_X.fit_transform(df[feature_cols])

# =========================
# Train / test split (time-aware)
# =========================
train_size = int(len(X_scaled) * 0.75)

X_train = X_scaled[:train_size]
y_train = y_scaled[:train_size]

X_test  = X_scaled[train_size:]
y_test  = y_scaled[train_size:]

# =========================
# Create sequences
# =========================
look_back = 24

trainX, trainY = create_exogenous_sequences(
    X_train, y_train, look_back
)

testX, testY = create_exogenous_sequences(
    X_test, y_test, look_back
)

# =========================
# Model
# =========================
model = Sequential([
    LSTM(64, return_sequences=True,
         input_shape=(look_back, X_scaled.shape[1])),
    LSTM(32),
    Dense(32, activation="relu"),
    Dense(1)
])

model.compile(
    optimizer="adam",
    loss=tf.keras.losses.Huber(delta=0.1),
    metrics=["mae"]
)

model.summary()

# =========================
# Train
# =========================
history = model.fit(
    trainX, trainY,
    epochs=50,
    batch_size=32,
    validation_data=(testX, testY),
    verbose=2
)

# Predict test set
y_test_pred_scaled = model.predict(testX, verbose=0)
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled)

y_test_true = scaler_y.inverse_transform(
    y_test.reshape(-1,1)
)

plt.figure(figsize=(14,4))
plt.plot(y_test_true[:500], label="Actual")
plt.plot(y_test_pred[:500], label="Predicted")
plt.legend()
plt.title("Forecast-style Test Evaluation")
plt.show()

# ---------------------------
# FINAL METRICS (after training)
# ---------------------------
print("Final Train Huber:", history.history["loss"][-1])
print("Final Val   Huber:", history.history["val_loss"][-1])
print("Final Train MAE  :", history.history["mae"][-1])
print("Final Val   MAE  :", history.history["val_mae"][-1])

# ---------------------------
# PLOTS
# ---------------------------
plt.figure(figsize=(10,4))
plt.plot(history.history["loss"], label="train_loss")
plt.plot(history.history["val_loss"], label="val_loss")
plt.legend()
plt.title("Huber Loss (Train vs Val)")
plt.show()

plt.figure(figsize=(10,4))
plt.plot(history.history["mae"], label="train_mae")
plt.plot(history.history["val_mae"], label="val_mae")
plt.legend()
plt.title("MAE (Train vs Val)")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# =========================
# Load forecast data
# =========================
df_f = pd.read_csv("Data_2025_seasonal_filled.csv")
df_f["time"] = pd.to_datetime(df_f["time"])

df_f["gti_raw"] = df_f["gti"].clip(lower=0)

# =========================
# Time features (MUST MATCH)
# =========================
df_f["hour"] = df_f["time"].dt.hour
df_f["doy"]  = df_f["time"].dt.dayofyear

df_f["hour_sin"] = np.sin(2*np.pi*df_f["hour"]/24)
df_f["hour_cos"] = np.cos(2*np.pi*df_f["hour"]/24)
df_f["doy_sin"]  = np.sin(2*np.pi*df_f["doy"]/365)
df_f["doy_cos"]  = np.cos(2*np.pi*df_f["doy"]/365)

df_f = df_f.drop(columns=["hour", "doy"])

# =========================
# GTI preprocessing (MATCH TRAIN)
# =========================
df_f["gti_log"] = np.log1p(df_f["gti_raw"])
df_f = df_f.replace([np.inf, -np.inf], np.nan).ffill().bfill()

# =========================
# Feature scaling
# =========================
X_2025 = scaler_X.transform(df_f[feature_cols])

# =========================
# Create sequences (NO dummy targets)
# =========================
X_2025_seq, _ = create_exogenous_sequences(
    X_2025,
    np.zeros(len(X_2025)),
    look_back
)

# =========================
# Predict
# =========================
P_scaled = model.predict(X_2025_seq, verbose=0)
P_norm_pred = scaler_y.inverse_transform(P_scaled).flatten()

# =========================
# Soft night physics
# =========================
gti = df_f["gti_raw"].iloc[look_back:].values
night_scale = np.clip(gti / 25.0, 0, 1)
P_norm_pred *= night_scale

P_norm_pred = np.clip(P_norm_pred, 0, 1)

# =========================
# Output
# =========================
df_out = pd.DataFrame({
    "time": df_f["time"].iloc[look_back:].values,
    "temp_F": df_f["temp_F"].iloc[look_back:].values,
    "gti": gti,
    "P_norm_pred": P_norm_pred
})

df_out.to_csv("2025_forecast_correct.csv", index=False)

# =========================
# Plot
# =========================
plt.figure(figsize=(14,5))
plt.plot(df_out["time"][:1000], df_out["P_norm_pred"][:1000])
plt.title("2025 Exogenous-only Solar Forecast")
plt.xlabel("Time")
plt.ylabel("P_norm")
plt.tight_layout()
plt.show()

plt.figure(figsize=(16,6))
plt.plot(df_out["time"], df_out["P_norm_pred"], linewidth=0.8)
plt.title("Predicted Solar Power for Full Year 2025", fontsize=14)
plt.xlabel("Date", fontsize=12)
plt.ylabel("Power (normalized)", fontsize=12)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

monthly_avg = df_out.resample("M", on="time")["P_norm_pred"].mean()

plt.figure(figsize=(12,5))
plt.plot(monthly_avg.index, monthly_avg.values, marker="o")
plt.title("Monthly Average Predicted Power - 2025")
plt.xlabel("Month")
plt.ylabel("Power (normalized)")
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

